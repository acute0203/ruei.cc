---
layout: post
title: 機器學習 百日馬拉松
date: 2019-04-18 19:44:37
tags: [Data Mining, Data Science, A.I.,Deep Learning]
---
本篇為紀錄參與百日馬拉松的主題及內容整理。

<!--more-->

[資料介紹與評估指標](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_001_HW.ipynb)

[EDA-1/讀取資料EDA: Data summary](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_002_HW.ipynb)

[3-1如何新建一個 dataframe?3-2 如何讀取其他資料? (非 csv 的資料)](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_003-2_HW.ipynb)

[EDA: 欄位的資料類型介紹及處理](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_004_HW.ipynb)

[EDA: 資料分佈](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_005_HW.ipynb)

[EDA: Outlier 及處理](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_006_HW.ipynb)

[常用的數值取代：中位數與分位數連續數值標準化](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_007_HW.ipynb)

[DataFrame operationData frame merge/常用的 DataFrame 操作](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_008_HW.ipynb)

[EDA from Correlation](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_010_HW.ipynb)

[EDA: 不同數值範圍間的特徵如何檢視/繪圖與樣式Kernel Density Estimation (KDE)](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_011_HW.ipynb)

[把連續的變數離散化](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_013_HW.ipynb)

[Subplots](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_014_HW.ipynb)

[Heatmap & Grid-plot](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_015_HW.ipynb)

[模型初體驗 Logistic Regression](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_016_HW.png)

[特徵工程簡介](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_017_HW.ipynb)

[特徵類型](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_018_HW.ipynb)

[數值型特徵 -  補缺失值與標準化](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_019_HW.ipynb)

[數值型特徵 -  去除離群值](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_020_HW.ipynb)

[數值型特徵 -  去除偏態](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_021_HW.ipynb)

[類別型特徵 - 基礎處理](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_022_HW.ipynb)

[類別型特徵 - 均值編碼](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_023_HW.ipynb)

[類別型特徵 - 其他進階處理](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_024_HW.ipynb)

[時間型特徵](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_025_HW.ipynb)

[特徵組合 - 數值與數值組合](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_026_HW.ipynb)

[特徵組合 - 類別與數值組合](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_027_HW.ipynb)

[特徵選擇](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_028_HW.ipynb)

[特徵評估](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_029_HW.ipynb)

[分類型特徵優化 - 葉編碼](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_030_HW.ipynb)

[機器學習概論](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_031_HW.ipynb)

[機器學習-流程與步驟](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_032_HW.ipynb)

[機器如何學習?](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_033_HW.ipynb)

[訓練/測試集切分的概念](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_034_HW.ipynb)

[regression vs. classification](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_035_HW.ipynb)

[評估指標選定/evaluation metrics](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_036_HW.ipynb)

[regression model 介紹 - 線性迴歸/羅吉斯回歸](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_037_HW.ipynb)

[regression model 程式碼撰寫](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_038_HW.ipynb)

[regression model 介紹 - LASSO 回歸/ Ridge 回歸](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_039_HW.ipynb)

[regression model 程式碼撰寫](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_040_HW.ipynb)

[tree based model - 決策樹 (Decision Tree) 模型介紹](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_041_HW.ipynb)

[tree based model - 決策樹程式碼撰寫](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_042_HW.ipynb)

[tree based model - 隨機森林 (Random Forest) 介紹](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_043_HW.ipynb)

[tree based model - 隨機森林程式碼撰寫](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_044_HW.ipynb)

[tree based model - 梯度提升機程式碼撰寫](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_048_HW.png)

[超參數調整與優化](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_047_HW.ipynb)

[Kaggle 競賽平台介紹](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_048_HW.png)

[Kaggle第一次期中考 考ML與調參相關](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_49_HW.png)

[clustering 1 非監督式機器學習簡介](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_054_HW.ipynb)

[clustering 2 聚類算法](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_055_HW.ipynb)

[clustering 2 聚類算法](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_056_HW.ipynb)

[clustering 3 階層分群算法](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_057_HW.ipynb)

[clustering 4 階層分群法](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_058_HW.ipynb)

[dimension reduction 1 降維方法-主成份分析](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_059_HW.ipynb)

[dimension reduction 1 降維方法-主成份分析](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_060_HW.ipynb)

[dimension reduction 2 降維方法-T-SNE](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_061_HW.ipynb)

[dimension reduction 2 降維方法-T-SNE](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_062_HW.ipynb)

[神經網路介紹](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_63_HW.ipynb)

[深度學習體驗 : 模型調整與學習曲線](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_064_HW.ipynb)

[深度學習體驗 : 啟動函數與正規化](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_065_HW.ipynb)

[Keras 安裝與介紹](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day_066_HW.ipynb)

[Keras Dataset](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day67-Keras_Dataset_HW.ipynb)

[Keras Sequential API](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day68-Keras_Sequential_model_HW.ipynb)

[Keras Module API](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day69-keras_model_api_HW.ipynb)

[Multi-layer Perception多層感知](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day70-Keras_Mnist_MLP_h256_HW.ipynb)

[損失函數](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day71-%E4%BD%BF%E7%94%A8%E6%90%8D%E5%A4%B1%E5%87%BD%E6%95%B8_HW.ipynb)

[激活函數](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day72-Activation_function_HW.ipynb)

[Gradient Descent](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day73_Gradient_Descent_HW.ipynb)

[Gradient Descent 數學原理](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day74-Gradient_Descent_HW.ipynb)

[BackPropagation](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day75-Back_Propagation_HW.ipynb)

[optimizers](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day76-Optimizer_HW.ipynb)

[訓練神經網路的細節與技巧 - Validation and overfit](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day077_HW.ipynb)

[訓練神經網路前的注意事項](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day078_HW.ipynb)

[訓練神經網路的細節與技巧 - Learning rate effect](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day079_HW.ipynb)

[[練習 Day] Compare different combinations of optimizers & learning rates](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day080_HW.ipynb)

[訓練神經網路的細節與技巧 - Regularization](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day081_HW.ipynb)

[訓練神經網路的細節與技巧 - Dropout](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day082_HW.ipynb)

[訓練神經網路的細節與技巧 - Batch normalization](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day083_HW.ipynb)

[[練習 Day] Comparing combinations of Activation function, optimizer and batch_norm or not](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day084_HW.ipynb)

[訓練神經網路的細節與技巧 - 使用 callbacks 函數做 earlystop](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day085_HW.ipynb)

[訓練神經網路的細節與技巧 - 使用 callbacks 函數儲存 model](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day086_HW.ipynb)

[訓練神經網路的細節與技巧 - 使用 callbacks 函數做 reduce learning rate](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day087HW.ipynb)

[訓練神經網路的細節與技巧 - 撰寫自己的 callbacks 函數](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day088_HW.ipynb)

[訓練神經網路的細節與技巧 - 撰寫自己的 Loss function](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day089_HW.ipynb)

[使用傳統電腦視覺與機器學習進行影像辨識](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day090_color_histogram_HW.ipynb)

[[練習 Day] 使用傳統電腦視覺與機器學習進行影像辨識](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day091_classification_with_cv_HW.ipynb)

[卷積神經網路 (Convolution Neural Network, CNN) 簡介](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day092_CNN_theory.ipynb)

[卷積神經網路架構細節](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/D93-CNN_Brief_HW.ipynb)

[卷積神經網路 - 卷積(Convolution)層與參數調整](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day94-CNN_Convolution_HW.ipynb)

[卷積神經網路 - 池化(Pooling)層與參數調整](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day95-CNN_Pooling_Padding_HW.ipynb)

[Keras 中的 CNN layers](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day096_Keras_CNN_layers.ipynb)

[使用 CNN 完成 CIFAR-10 資料集](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day097_Keras_CNN_vs_DNN.ipynb)

[訓練卷積神經網路的細節與技巧 - 處理大量數據](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day098_Python_generator.ipynb)

[訓練卷積神經網路的細節與技巧 - 處理小量數據](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day099_data_augmentation.ipynb)

[訓練卷積神經網路的細節與技巧 - 轉移學習 (Transfer learning)](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day100_transfer_learning_HW.ipynb)

[影像辨識](https://github.com/acute0203/100Day-ML-Marathon/blob/master/Homework/Day101-105_HW.png)
